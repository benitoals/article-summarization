{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: peft in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (0.29.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/metal/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate peft huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import RepositoryNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/anaconda3/envs/metal/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "# Model and dataset IDs\n",
    "model_name = \"google/mt5-small\"\n",
    "local_dataset_id = \"benitoals/my-txt-dataset\"\n",
    "hf_dataset_id = \"CShorten/ML-ArXiv-Papers\"\n",
    "\n",
    "local_model_repo_id = \"benitoals/my-lora\"\n",
    "hf_model_repo_id = \"benitoals/my-lora-hf\"\n",
    "combined_model_repo_id = \"benitoals/my-lora-local-combined\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, body_key, summary_key, max_input_len=512, max_target_len=256):\n",
    "    inputs = examples[body_key]\n",
    "    targets = examples[summary_key]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_len, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d9a375114041f484e4f8f35dadc9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6eb45c031c4c889fc41876c60a657f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeec8bf607814e11b5859098324ad33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'summary', 'body'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['filename', 'summary', 'body'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'summary', 'body'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n",
      "Hugging Face dataset loaded: Dataset({\n",
      "    features: ['Unnamed: 0.1', 'Unnamed: 0', 'title', 'abstract'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "local_data = load_dataset(local_dataset_id)\n",
    "\n",
    "# Filtering short summaries\n",
    "local_data = local_data.filter(lambda x: len(x[\"summary\"].split()) >= 50)\n",
    "print(\"Local dataset loaded:\", local_data)\n",
    "\n",
    "hf_data = load_dataset(hf_dataset_id, split=\"train\").shuffle(seed=42).select(range(1000))\n",
    "print(\"Hugging Face dataset loaded:\", hf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora(base_model, dataset, tokenizer, model_repo_id, \n",
    "               body_key=\"body\", summary_key=\"summary\", \n",
    "               num_epochs=4, learning_rate=1e-4, skip_if_hf_exists=True,\n",
    "               freeze_base=False):\n",
    "    \"\"\"Fine-tunes a model using LoRA, checks HF repo to skip training if already exists,\n",
    "       and optionally freezes the base model parameters (non-adapter) before training.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "    # Check if model exists on Hugging Face\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        info = api.repo_info(model_repo_id, repo_type=\"model\")\n",
    "        print(f\"\\n[Skipping Training?] {model_repo_id} found on HF. Checking for adapter config...\")\n",
    "        \n",
    "        # This will try to load the adapter config and weights.\n",
    "        loaded_lora_model = PeftModel.from_pretrained(base_model, model_repo_id)\n",
    "        print(\"\\n=== LoRA Model Successfully Loaded ===\")\n",
    "        # print(loaded_lora_model)  # Debug info\n",
    "        print(f\"Found LoRA adapter in {model_repo_id}, skipping training.\")\n",
    "        \n",
    "        loaded_lora_model.to(device)\n",
    "        return loaded_lora_model\n",
    "    except (RepositoryNotFoundError, ValueError, OSError) as e:\n",
    "        print(f\"HF repo {model_repo_id} found but no valid LoRA adapter inside (or missing adapter_config.json).\")\n",
    "        print(f\"Proceeding with training. Error was: {e}\")\n",
    "\n",
    "    base_model.to(device)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=1, lora_alpha=16, lora_dropout=0.2,\n",
    "        target_modules=[\"q\", \"v\"]\n",
    "    )\n",
    "    lora_model = get_peft_model(base_model, peft_config).to(device)\n",
    "\n",
    "    # If freeze_base is True, freeze all parameters except those related to LoRA\n",
    "    if freeze_base:\n",
    "        for name, param in lora_model.named_parameters():\n",
    "            if \"lora_\" not in name:\n",
    "                param.requires_grad = False\n",
    "        print(\"Base model parameters frozen. Only LoRA adapter parameters will be updated.\")\n",
    "\n",
    "    # Handle dataset splits: if dataset is not a dict, create splits\n",
    "    if isinstance(dataset, dict):\n",
    "        train_ds = dataset[\"train\"]\n",
    "        eval_ds = dataset[\"validation\"]\n",
    "        test_ds = dataset[\"test\"]\n",
    "    else:\n",
    "        splits = dataset.train_test_split(test_size=0.2)\n",
    "        eval_test = splits[\"test\"].train_test_split(test_size=0.5)\n",
    "        train_ds = splits[\"train\"]\n",
    "        eval_ds = eval_test[\"train\"]\n",
    "        test_ds = eval_test[\"test\"]\n",
    "\n",
    "    # Tokenize each split separately using their own column names\n",
    "    def tokenize_dataset(ds):\n",
    "        return ds.map(lambda x: preprocess_function(x, tokenizer, body_key, summary_key),\n",
    "                      batched=True,\n",
    "                      remove_columns=ds.column_names)\n",
    "    \n",
    "    tokenized_train = tokenize_dataset(train_ds)\n",
    "    tokenized_eval = tokenize_dataset(eval_ds)\n",
    "    tokenized_test = tokenize_dataset(test_ds)\n",
    "    \n",
    "    # Prepare a dictionary for consistency\n",
    "    tokenized_ds = {\"train\": tokenized_train, \"validation\": tokenized_eval, \"test\": tokenized_test}\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=lora_model, label_pad_token_id=-100)\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        # Fix shape issues\n",
    "        if preds.ndim == 3 and preds.shape[1] == 1:\n",
    "            preds = np.squeeze(preds, axis=1)\n",
    "        if labels.ndim == 3 and labels.shape[1] == 1:\n",
    "            labels = np.squeeze(labels, axis=1)\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "        labels = np.clip(labels, 0, tokenizer.vocab_size - 1)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        result = evaluate.load(\"rouge\").compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        if isinstance(result[\"rouge1\"], float):\n",
    "            return {k: v * 100 for k, v in result.items()}\n",
    "        return {k: v.mid.fmeasure * 100 for k, v in result.items()}\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"model_lora_temp\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=learning_rate,\n",
    "        max_grad_norm=0.1,\n",
    "        eval_steps=5,\n",
    "        save_steps=5,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=5,\n",
    "        push_to_hub=True,\n",
    "        hub_model_id=model_repo_id,\n",
    "        hub_strategy=\"end\",\n",
    "        report_to=[\"tensorboard\"]\n",
    "    )\n",
    "\n",
    "    trainer = CustomSeq2SeqTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Start LoRA Fine-tuning on {model_repo_id} ===\")\n",
    "    trainer.train()\n",
    "    print(\"=== LoRA Fine-tuning complete ===\")\n",
    "\n",
    "    # Save LoRA weights locally and push to Hugging Face\n",
    "    trainer.save_model()\n",
    "    lora_model.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    final_eval = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "    print(\"Trainer Evaluate (test set):\", final_eval)\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_scores(model, dataset, tokenizer, device, body_key=\"body\", summary_key=\"summary\", max_length=128, num_beams=3):\n",
    "    \"\"\"Evaluate a model by generating summaries and comparing with reference summaries.\n",
    "       Uses bad_words_ids to prevent generation of <extra_id_0>.\"\"\"\n",
    "    debug = True\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    preds, refs = [], []\n",
    "    \n",
    "    # Get the token id for <extra_id_0>\n",
    "    bad_token_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "    bad_words = [[bad_token_id]]\n",
    "\n",
    "    for i, ex in enumerate(dataset):\n",
    "        body_text = ex[body_key]\n",
    "        ref_text  = ex[summary_key]\n",
    "        if not body_text.strip():\n",
    "            preds.append(\"\")\n",
    "            refs.append(ref_text)\n",
    "            continue\n",
    "\n",
    "        input_ids = tokenizer.encode(\"summarize: \" + body_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,  # Avoid repeated words\n",
    "            do_sample=True,  # Use deterministic generation\n",
    "            temperature=0.7,  # Ensure stable output\n",
    "            top_k=50,  # Prevent degenerate outputs\n",
    "            top_p=0.95, # Ensure diverse summaries\n",
    "            bad_words_ids=bad_words \n",
    "        )\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        preds.append(pred_text)\n",
    "        refs.append(ref_text)\n",
    "\n",
    "        # Print a few debug examples\n",
    "        if debug and i < 3:\n",
    "            print(f\"\\n--- Debug Example {i} ---\")\n",
    "            print(\"Input (first 300 chars):\", body_text[:300])\n",
    "            print(\"Predicted Summary:\", pred_text)\n",
    "            print(\"Reference Summary:\", ref_text)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    result = rouge.compute(predictions=preds, references=refs)\n",
    "    # Convert floats to percentages if needed\n",
    "    if isinstance(result[\"rouge1\"], float):\n",
    "        return {k: v * 100 for k, v in result.items()}\n",
    "    return {k: v.mid.fmeasure * 100 for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Four Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline (Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: <extra_id_0>- tionDirect.com\n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: <extra_id_0> of PLC and field devices\n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: <extra_id_0> .\n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "Baseline ROUGE Scores: {'rouge1': 1.0774674150441677, 'rouge2': 0.1993950644528968, 'rougeL': 0.963159334148876, 'rougeLsum': 0.9413942392291867}\n"
     ]
    }
   ],
   "source": [
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "baseline_rouge = get_rouge_scores(baseline_model, local_data[\"test\"], tokenizer, device)\n",
    "print(\"Baseline ROUGE Scores:\", baseline_rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA on Local Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Skipping Training?] benitoals/my-lora found on HF. Checking for adapter config...\n",
      "\n",
      "=== LoRA Model Successfully Loaded ===\n",
      "Found LoRA adapter in benitoals/my-lora, skipping training.\n",
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: <extra_id_0> PLCs (PLC)s. The PLC uses a control-logic program to control a physical process (PLC). The software uses an authentication protocol. The software is based on the PLC (PLC), which allows PLC to control the control- logic. These systems are known as PLC. However, PLC is responsible for controlling a system (PLCs) to analyze the remote control systems (ICS) and PLC systems. This PLC can be controlled by PLC-s. This paper presents an\n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: <extra_id_0> of SCADAs and PLCs. In this case, we need to identify SCADA systems. However, there is a few examples of PLC attacks. The PLC is very important to avoid SCADA attacks, such as SCADA and SCADA. This is known as the PLC. These systems are known to be controlled by SCADA (SCADA) systems. It is expected to be attacked by PLC systems. The SCADA is critical to attack the control systems. We need to ensure that SCADA system is \n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: <extra_id_0>.)\n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "Local LoRA ROUGE Scores: {'rouge1': 17.739976344864303, 'rouge2': 3.3456048015832933, 'rougeL': 10.64148322021589, 'rougeLsum': 10.635059660999952}\n"
     ]
    }
   ],
   "source": [
    "local_lora_model = train_lora(baseline_model, local_data, tokenizer, local_model_repo_id)\n",
    "local_lora_rouge = get_rouge_scores(local_lora_model, local_data[\"test\"], tokenizer, device)\n",
    "print(\"Local LoRA ROUGE Scores:\", local_lora_rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA on HF Science Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Skipping Training?] benitoals/my-lora-hf found on HF. Checking for adapter config...\n",
      "\n",
      "=== LoRA Model Successfully Loaded ===\n",
      "Found LoRA adapter in benitoals/my-lora-hf, skipping training.\n",
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: <extra_id_0> a - - the a- s - of a system that uses a method of authentication . using a software using the software. a. s. based algorithm. which is connected with the algorithm of the PLC. where their authentication and authentication in a computer based system. . and a the .. - and which the e- n e . the s in the implementation of these algorithms.., which allows \n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: <extra_id_0> of the a- - which is a . - of the system which allows the e- s . and the s of the control system. which it is useful. the ability of a method of which the implementation of where the algorithm is limited. using a software based system a system . in the system of us based systems based on a data based algorithm. . of the method which and where it is used. the system is known of the systems \n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: <extra_id_0> , , and the physical infrastructure, which is using physical systems based on computing and infrastructure based computing systems, including a physical system based in the s - which have developed a network that which uses physical and physical technology, and their s- . which are connected and connected with the e-s s and s of the infrastructure and a the ability of their systems and computing infrastructure. , the capacity of using the algorithm\n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "HF LoRA ROUGE Scores: {'rouge1': 20.488005267570514, 'rouge2': 2.5539432355386875, 'rougeL': 12.655852579622412, 'rougeLsum': 12.624319754899027}\n"
     ]
    }
   ],
   "source": [
    "hf_lora_model = train_lora(baseline_model, hf_data, tokenizer, hf_model_repo_id, \n",
    "                           body_key=\"title\", summary_key=\"abstract\")\n",
    "hf_lora_rouge = get_rouge_scores(hf_lora_model, local_data[\"test\"], tokenizer, device)\n",
    "print(\"HF LoRA ROUGE Scores:\", hf_lora_rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HF + Local Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Skipping Training?] benitoals/my-lora-local-combined found on HF. Checking for adapter config...\n",
      "\n",
      "=== LoRA Model Successfully Loaded ===\n",
      "Found LoRA adapter in benitoals/my-lora-local-combined, skipping training.\n",
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: <extra_id_0> logic (PLCs) are used in PLCs. In industrial control systems (ICS), the PLC uses a control-logic program (PLC). The PLC is based on a physical process (PLC) using a software based PLC. They operate remotely. These systems rely on PLC-s. The software is designed to control the physical processes. The ICS uses the control logic based control- logic (ICs) to control physical control systems. In ICS, the software uses software-based software\n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: <extra_id_0> of Intelligent Systems <extra_id_1>.com. However, there is a lot of attacks that can be attacked by SCADAs. The typical control systems are controlled by PLCs. It is very difficult to detect SCADA systems. In this context, we need a few attacks. These attacks are very similar to SCADA, which is known as SCADA. This vulnerability could be used by remote control systems. We need to avoid attacks against SCADA attacks (SCADAs). This is an important feature of\n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: <extra_id_0> physical systems (CPSs) is a critical vulnerability that could be enhanced by software attacks, which could have been threatened by critical systems, such as infrastructure infrastructure, including infrastructure and infrastructure. This work is supported by the physical system attacks. In this paper, we are considered that the CPSs are being controlled by software, based on the IoT attacks and IoT systems are known as cyber physical Systems, where it is critical for physical infrastructures. The \n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "Combined (HF+Local) ROUGE Scores: {'rouge1': 22.14484071517685, 'rouge2': 4.027771537736193, 'rougeL': 12.799231904358338, 'rougeLsum': 12.749814557947017}\n"
     ]
    }
   ],
   "source": [
    "combined_lora_model = train_lora(hf_lora_model, local_data, tokenizer, combined_model_repo_id, freeze_base=True)\n",
    "combined_lora_rouge = get_rouge_scores(combined_lora_model, local_data[\"test\"], tokenizer, device)\n",
    "print(\"Combined (HF+Local) ROUGE Scores:\", combined_lora_rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/metal/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, text, device, max_length=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "def evaluate_summary(pred, ref):\n",
    "    scores = rouge.compute(predictions=[pred], references=[ref])\n",
    "    return {k: round(v * 100, 2) for k, v in scores.items()}\n",
    "\n",
    "def load_model(base_model_name, repo_id, device):\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name).to(device)\n",
    "    if repo_id:\n",
    "        try:\n",
    "            model = PeftModel.from_pretrained(base_model, repo_id).to(device)\n",
    "            print(f\"Loaded LoRA adapter from {repo_id}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading LoRA adapter from {repo_id}: {e}. Using base model instead.\")\n",
    "            return base_model\n",
    "    else:\n",
    "        print(\"Using baseline pretrained model\")\n",
    "        return base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Baseline (Pretrained)...\n",
      "Using baseline pretrained model\n",
      "\n",
      "Loading LoRA Local...\n",
      "Loaded LoRA adapter from benitoals/my-lora\n",
      "\n",
      "Loading HF Science Dataset...\n",
      "Loaded LoRA adapter from benitoals/my-lora-hf\n",
      "\n",
      "Loading HF + Local Fine-tuned...\n",
      "Loaded LoRA adapter from benitoals/my-lora-local-combined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/metal/lib/python3.12/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight', 'base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "model_repos = {\n",
    "    \"Baseline (Pretrained)\": None,\n",
    "    \"LoRA Local\": \"benitoals/my-lora\",\n",
    "    \"HF Science Dataset\": \"benitoals/my-lora-hf\",\n",
    "    \"HF + Local Fine-tuned\": \"benitoals/my-lora-local-combined\"\n",
    "}\n",
    "\n",
    "models = {}\n",
    "\n",
    "for model_label, repo_id in model_repos.items():\n",
    "    print(f\"\\nLoading {model_label}...\")\n",
    "    models[model_label] = load_model(model_name, repo_id, device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Input and Reference from .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded input_text:\n",
      " I. I NTRODUCTION\n",
      "Processors are becoming more and more important in the\n",
      "world. Today processors can be seen almost everywhere and in\n",
      "the future they will only become more abundant. There are also\n",
      "many types of computers such microcontrollers, distributed\n",
      "control systems (DCSs), and regular personal computers.\n",
      "However, most factories use programmable logic controllers\n",
      "(PLCs) for system control. While these are not very fast, they\n",
      "are very reliable, being able to run 24 hours a day, seven days a\n",
      "w ...\n",
      "\n",
      "Loaded abstract as reference_summary:\n",
      " Formal Veriﬁcation of Ladder\n",
      "Logic programs using NuSMV\n",
      "Sam Kottler\u0003, Mehdy Khayamy,ySyed Rafay Hasan,zand Omar Elkeelanyx\n",
      "\u0003Colorado College, Department of Mathematics and Computer Science, sam.kottler@coloradocollege.edu\n",
      "y z xTennessee Technological University, Electrical and Computer Engineering D ...\n"
     ]
    }
   ],
   "source": [
    "# Load input_text from file\n",
    "with open('input_text.txt', 'r', encoding='utf-8') as f:\n",
    "    input_text = f.read().strip()\n",
    "\n",
    "# Load reference_summary from file\n",
    "with open('abstract.txt', 'r', encoding='utf-8') as f:\n",
    "    reference_summary = f.read().strip()\n",
    "\n",
    "print(\"Loaded input_text:\\n\", input_text[:500], \"...\\n\")\n",
    "print(\"Loaded abstract as reference_summary:\\n\", reference_summary[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline (Pretrained) ===\n",
      "\n",
      "Generated Summary:\n",
      " <extra_id_0> could interfere.\n",
      "\n",
      "Reference Abstract:\n",
      " Formal Veriﬁcation of Ladder\n",
      "Logic programs using NuSMV\n",
      "Sam Kottler\u0003, Mehdy Khayamy,ySyed Rafay Hasan,zand Omar Elkeelanyx\n",
      "\u0003Colorado College, Department of Mathematics and Computer Science, sam.kottler@coloradocollege.edu\n",
      "y z xTennessee Technological University, Electrical and Computer Engineering Department\n",
      "ymkhayamy42@students.tntech.edu,zshasan@tntech.edu,xoelkeelany@tntech.edu\n",
      "Abstract —Programmable logic controllers (PLCs) are heavy-\n",
      "duty computers used to control industrial systems. For many\n",
      "years these systems were physically separated from any other\n",
      "network making attacks extremely difﬁcult. However, these in-\n",
      "creasingly connected systems have not improved much in terms of\n",
      "security, leaving them vulnerable to attacks. This paper attempts\n",
      "to show that ladder logic programs for PLCs can be modeled\n",
      "in NuSMV and veriﬁed using computational tree logic (CTL)\n",
      "speciﬁcations. This paper also shows how simple changes to the\n",
      "ladder logic program can cause catastrophic damage to the PLC\n",
      "system. This intruded code can be difﬁcult to detect by looking at\n",
      "the ladder logic program because the change is so small. However,\n",
      "the intruded code can be modeled in NuSMV and identiﬁed by\n",
      "properly written CTL speciﬁcations.\n",
      "\n",
      "ROUGE Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== LoRA Local ===\n",
      "\n",
      "Generated Summary:\n",
      " <extra_id_0>.\n",
      "\n",
      "Reference Abstract:\n",
      " Formal Veriﬁcation of Ladder\n",
      "Logic programs using NuSMV\n",
      "Sam Kottler\u0003, Mehdy Khayamy,ySyed Rafay Hasan,zand Omar Elkeelanyx\n",
      "\u0003Colorado College, Department of Mathematics and Computer Science, sam.kottler@coloradocollege.edu\n",
      "y z xTennessee Technological University, Electrical and Computer Engineering Department\n",
      "ymkhayamy42@students.tntech.edu,zshasan@tntech.edu,xoelkeelany@tntech.edu\n",
      "Abstract —Programmable logic controllers (PLCs) are heavy-\n",
      "duty computers used to control industrial systems. For many\n",
      "years these systems were physically separated from any other\n",
      "network making attacks extremely difﬁcult. However, these in-\n",
      "creasingly connected systems have not improved much in terms of\n",
      "security, leaving them vulnerable to attacks. This paper attempts\n",
      "to show that ladder logic programs for PLCs can be modeled\n",
      "in NuSMV and veriﬁed using computational tree logic (CTL)\n",
      "speciﬁcations. This paper also shows how simple changes to the\n",
      "ladder logic program can cause catastrophic damage to the PLC\n",
      "system. This intruded code can be difﬁcult to detect by looking at\n",
      "the ladder logic program because the change is so small. However,\n",
      "the intruded code can be modeled in NuSMV and identiﬁed by\n",
      "properly written CTL speciﬁcations.\n",
      "\n",
      "ROUGE Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== HF Science Dataset ===\n",
      "\n",
      "Generated Summary:\n",
      " <extra_id_0> which is very efficient and efficient. However, it would increase the capacity of the PLC which could cause a certain system which can cause problems in PLC programs. These would have been very important in a system where the remote which the a. which it could have a problem. Although it is limited because it is not necessary, it is the ability to operate in these types of systems which are being used in the various processes. The \n",
      "\n",
      "Reference Abstract:\n",
      " Formal Veriﬁcation of Ladder\n",
      "Logic programs using NuSMV\n",
      "Sam Kottler\u0003, Mehdy Khayamy,ySyed Rafay Hasan,zand Omar Elkeelanyx\n",
      "\u0003Colorado College, Department of Mathematics and Computer Science, sam.kottler@coloradocollege.edu\n",
      "y z xTennessee Technological University, Electrical and Computer Engineering Department\n",
      "ymkhayamy42@students.tntech.edu,zshasan@tntech.edu,xoelkeelany@tntech.edu\n",
      "Abstract —Programmable logic controllers (PLCs) are heavy-\n",
      "duty computers used to control industrial systems. For many\n",
      "years these systems were physically separated from any other\n",
      "network making attacks extremely difﬁcult. However, these in-\n",
      "creasingly connected systems have not improved much in terms of\n",
      "security, leaving them vulnerable to attacks. This paper attempts\n",
      "to show that ladder logic programs for PLCs can be modeled\n",
      "in NuSMV and veriﬁed using computational tree logic (CTL)\n",
      "speciﬁcations. This paper also shows how simple changes to the\n",
      "ladder logic program can cause catastrophic damage to the PLC\n",
      "system. This intruded code can be difﬁcult to detect by looking at\n",
      "the ladder logic program because the change is so small. However,\n",
      "the intruded code can be modeled in NuSMV and identiﬁed by\n",
      "properly written CTL speciﬁcations.\n",
      "\n",
      "ROUGE Scores: {'rouge1': 20.9, 'rouge2': 1.5, 'rougeL': 9.7, 'rougeLsum': 18.66}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== HF + Local Fine-tuned ===\n",
      "\n",
      "Generated Summary:\n",
      " <extra_id_0> could interfere.\n",
      "\n",
      "Reference Abstract:\n",
      " Formal Veriﬁcation of Ladder\n",
      "Logic programs using NuSMV\n",
      "Sam Kottler\u0003, Mehdy Khayamy,ySyed Rafay Hasan,zand Omar Elkeelanyx\n",
      "\u0003Colorado College, Department of Mathematics and Computer Science, sam.kottler@coloradocollege.edu\n",
      "y z xTennessee Technological University, Electrical and Computer Engineering Department\n",
      "ymkhayamy42@students.tntech.edu,zshasan@tntech.edu,xoelkeelany@tntech.edu\n",
      "Abstract —Programmable logic controllers (PLCs) are heavy-\n",
      "duty computers used to control industrial systems. For many\n",
      "years these systems were physically separated from any other\n",
      "network making attacks extremely difﬁcult. However, these in-\n",
      "creasingly connected systems have not improved much in terms of\n",
      "security, leaving them vulnerable to attacks. This paper attempts\n",
      "to show that ladder logic programs for PLCs can be modeled\n",
      "in NuSMV and veriﬁed using computational tree logic (CTL)\n",
      "speciﬁcations. This paper also shows how simple changes to the\n",
      "ladder logic program can cause catastrophic damage to the PLC\n",
      "system. This intruded code can be difﬁcult to detect by looking at\n",
      "the ladder logic program because the change is so small. However,\n",
      "the intruded code can be modeled in NuSMV and identiﬁed by\n",
      "properly written CTL speciﬁcations.\n",
      "\n",
      "ROUGE Scores: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model_label, model in models.items():\n",
    "    print(f\"\\n=== {model_label} ===\")\n",
    "    summary = generate_summary(model, tokenizer, input_text, device)\n",
    "    scores = evaluate_summary(summary, reference_summary)\n",
    "\n",
    "    print(\"\\nGenerated Summary:\\n\", summary)\n",
    "    print(\"\\nReference Abstract:\\n\", reference_summary)\n",
    "    print(\"\\nROUGE Scores:\", scores)\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
