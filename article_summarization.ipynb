{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Summarization Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove unwanted special tokens from text.\"\"\"\n",
    "    return text.replace(\"<extra_id_0>\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_generated_text(text):\n",
    "    \"\"\"Post-process generated text to remove unwanted tokens and extra whitespace.\"\"\"\n",
    "    text = text.replace(\"<extra_id_0>\", \"\").strip()\n",
    "    return \" \".join(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, body_key, summary_key, max_input_len=512, max_target_len=256, chunk_overlap=50):\n",
    "    \"\"\"Prepares dataset: Tokenizes input with chunking and cleans target summaries.\"\"\"\n",
    "    chunked_inputs = []\n",
    "    chunked_summaries = []\n",
    "\n",
    "    for body_text, summary_text in zip(examples[body_key], examples[summary_key]):        # Skip if summary is too short\n",
    "        if len(summary_text.split()) < 50:\n",
    "            continue\n",
    "\n",
    "        # Clean the summary text to remove unwanted tokens\n",
    "        summary_text = clean_text(summary_text)\n",
    "        tokenized_body = tokenizer(body_text, truncation=False)[\"input_ids\"]\n",
    "        body_chunks = [\n",
    "            tokenized_body[i : i + max_input_len]\n",
    "            for i in range(0, len(tokenized_body), max_input_len - chunk_overlap)\n",
    "        ]\n",
    "        tokenized_summary = tokenizer(summary_text, truncation=True, max_length=max_target_len)[\"input_ids\"]\n",
    "\n",
    "        for chunk in body_chunks:\n",
    "            chunked_inputs.append(chunk)\n",
    "            chunked_summaries.append(tokenized_summary)\n",
    "\n",
    "    return {\"input_ids\": chunked_inputs, \"labels\": chunked_summaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_scores(model, dataset, tokenizer, device, body_key=\"body\", summary_key=\"summary\", max_length=128, num_beams=3):\n",
    "    \"\"\"Evaluate a model by generating summaries and comparing with reference summaries.\n",
    "       Uses bad_words_ids to prevent generation of <extra_id_0>.\"\"\"\n",
    "    debug = True\n",
    "\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    preds, refs = [], []\n",
    "    \n",
    "    # Get the token id for <extra_id_0>\n",
    "    bad_token_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "    bad_words = [[bad_token_id]]\n",
    "\n",
    "    for i, ex in enumerate(dataset):\n",
    "        body_text = ex[body_key]\n",
    "        ref_text  = ex[summary_key]\n",
    "        if not body_text.strip():\n",
    "            preds.append(\"\")\n",
    "            refs.append(ref_text)\n",
    "            continue\n",
    "\n",
    "        input_ids = tokenizer.encode(\"summarize: \" + body_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,  # Avoid repeated words\n",
    "            do_sample=False,  # Use deterministic generation\n",
    "            temperature=0.7,  # Ensure stable output\n",
    "            top_k=50,  # Prevent degenerate outputs\n",
    "            top_p=0.95, # Ensure diverse summaries\n",
    "            bad_words_ids=bad_words \n",
    "        )\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred_text = post_process_generated_text(pred_text)\n",
    "        pred_text = post_process_generated_text(pred_text)\n",
    "        preds.append(pred_text)\n",
    "        refs.append(ref_text)\n",
    "\n",
    "        # Print a few debug examples\n",
    "        if debug and i < 3:\n",
    "            print(f\"\\n--- Debug Example {i} ---\")\n",
    "            print(\"Input (first 300 chars):\", body_text[:300])\n",
    "            print(\"Predicted Summary:\", pred_text)\n",
    "            print(\"Reference Summary:\", ref_text)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    result = rouge.compute(predictions=preds, references=refs)\n",
    "    # Convert floats to percentages if needed\n",
    "    if isinstance(result[\"rouge1\"], float):\n",
    "        return {k: v * 100 for k, v in result.items()}\n",
    "    return {k: v.mid.fmeasure * 100 for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"Custom Trainer to handle LoRA-specific issues.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora(base_model, dataset, tokenizer, model_repo_id, \n",
    "               body_key=\"body\", summary_key=\"summary\", \n",
    "               num_epochs=4, learning_rate=1e-4, skip_if_hf_exists=True,\n",
    "               freeze_base=False):\n",
    "    \"\"\"Fine-tunes a model using LoRA, checks HF repo to skip training if already exists,\n",
    "       and optionally freezes the base model parameters (non-adapter) before training.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "\n",
    "    # Check if model exists on Hugging Face\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        info = api.repo_info(model_repo_id, repo_type=\"model\")\n",
    "        print(f\"\\n[Skipping Training?] {model_repo_id} found on HF. Checking for adapter config...\")\n",
    "        \n",
    "        # This will try to load the adapter config and weights.\n",
    "        loaded_lora_model = PeftModel.from_pretrained(base_model, model_repo_id)\n",
    "        print(\"\\n=== LoRA Model Successfully Loaded ===\")\n",
    "        # print(loaded_lora_model)  # Debug info\n",
    "        print(f\"Found LoRA adapter in {model_repo_id}, skipping training.\")\n",
    "        \n",
    "        loaded_lora_model.to(device)\n",
    "        return loaded_lora_model\n",
    "    except (RepositoryNotFoundError, ValueError, OSError) as e:\n",
    "        print(f\"HF repo {model_repo_id} found but no valid LoRA adapter inside (or missing adapter_config.json).\")\n",
    "        print(f\"Proceeding with training. Error was: {e}\")\n",
    "\n",
    "    base_model.to(device)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=1, lora_alpha=16, lora_dropout=0.2,\n",
    "        target_modules=[\"q\", \"v\"]\n",
    "    )\n",
    "    lora_model = get_peft_model(base_model, peft_config).to(device)\n",
    "\n",
    "    # If freeze_base is True, freeze all parameters except those related to LoRA\n",
    "    if freeze_base:\n",
    "        for name, param in lora_model.named_parameters():\n",
    "            if \"lora_\" not in name:\n",
    "                param.requires_grad = False\n",
    "        print(\"Base model parameters frozen. Only LoRA adapter parameters will be updated.\")\n",
    "\n",
    "    # Handle dataset splits: if dataset is not a dict, create splits\n",
    "    if isinstance(dataset, dict):\n",
    "        train_ds = dataset[\"train\"]\n",
    "        eval_ds = dataset[\"validation\"]\n",
    "        test_ds = dataset[\"test\"]\n",
    "    else:\n",
    "        splits = dataset.train_test_split(test_size=0.2)\n",
    "        eval_test = splits[\"test\"].train_test_split(test_size=0.5)\n",
    "        train_ds = splits[\"train\"]\n",
    "        eval_ds = eval_test[\"train\"]\n",
    "        test_ds = eval_test[\"test\"]\n",
    "\n",
    "    # Tokenize each split separately using their own column names\n",
    "    def tokenize_dataset(ds):\n",
    "        return ds.map(lambda x: preprocess_function(x, tokenizer, body_key, summary_key),\n",
    "                      batched=True,\n",
    "                      remove_columns=ds.column_names)\n",
    "    \n",
    "    tokenized_train = tokenize_dataset(train_ds)\n",
    "    tokenized_eval = tokenize_dataset(eval_ds)\n",
    "    tokenized_test = tokenize_dataset(test_ds)\n",
    "    \n",
    "    # Prepare a dictionary for consistency\n",
    "    tokenized_ds = {\"train\": tokenized_train, \"validation\": tokenized_eval, \"test\": tokenized_test}\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=lora_model, label_pad_token_id=-100)\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        # Fix shape issues\n",
    "        if preds.ndim == 3 and preds.shape[1] == 1:\n",
    "            preds = np.squeeze(preds, axis=1)\n",
    "        if labels.ndim == 3 and labels.shape[1] == 1:\n",
    "            labels = np.squeeze(labels, axis=1)\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        preds = np.clip(preds, 0, tokenizer.vocab_size - 1)\n",
    "        labels = np.clip(labels, 0, tokenizer.vocab_size - 1)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        result = evaluate.load(\"rouge\").compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        if isinstance(result[\"rouge1\"], float):\n",
    "            return {k: v * 100 for k, v in result.items()}\n",
    "        return {k: v.mid.fmeasure * 100 for k, v in result.items()}\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"model_lora_temp\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=learning_rate,\n",
    "        max_grad_norm=0.1,\n",
    "        eval_steps=5,\n",
    "        save_steps=5,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=5,\n",
    "        push_to_hub=True,\n",
    "        hub_model_id=model_repo_id,\n",
    "        hub_strategy=\"end\",\n",
    "        report_to=[\"tensorboard\"]\n",
    "    )\n",
    "\n",
    "    trainer = CustomSeq2SeqTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Start LoRA Fine-tuning on {model_repo_id} ===\")\n",
    "    trainer.train()\n",
    "    print(\"=== LoRA Fine-tuning complete ===\")\n",
    "\n",
    "    # Save LoRA weights locally and push to Hugging Face\n",
    "    trainer.save_model()\n",
    "    lora_model.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    final_eval = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "    print(\"Trainer Evaluate (test set):\", final_eval)\n",
    "\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset and model repository IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/metal/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'summary', 'body'],\n",
      "        num_rows: 51\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['filename', 'summary', 'body'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filename', 'summary', 'body'],\n",
      "        num_rows: 11\n",
      "    })\n",
      "})\n",
      "Filtered dataset sizes - Train: 50, Validation: 11, Test: 11\n"
     ]
    }
   ],
   "source": [
    "dataset_repo_id = \"benitoals/my-txt-dataset\"\n",
    "model_name = \"google/mt5-xl\"\n",
    "local_model_repo_id = \"benitoals/my-lora\"\n",
    "hf_model_repo_id = \"benitoals/my-lora-hf\"\n",
    "combined_repo_id = \"benitoals/my-lora-local-combined\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, legacy=False)\n",
    "\n",
    "# Load pre-existing local dataset\n",
    "local_data = load_dataset(dataset_repo_id)\n",
    "print(\"Loaded dataset:\", local_data)\n",
    "\n",
    "# Filter out short summaries\n",
    "local_data = local_data.filter(lambda x: len(x[\"summary\"].split()) >= 50)\n",
    "if isinstance(local_data, dict):\n",
    "    print(f\"Filtered dataset sizes - Train: {len(local_data['train'])}, Validation: {len(local_data['validation'])}, Test: {len(local_data['test'])}\")\n",
    "else:\n",
    "    print(f\"Filtered dataset size: {len(local_data)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                            else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/metal/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/metal/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: tion\n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: :\n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: .\n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Baseline (Pretrained Model) Results ===\n",
      "{'rouge1': 0.2525137432388697, 'rouge2': 0.12794102714632688, 'rougeL': 0.2525137432388697, 'rougeLsum': 0.2525137432388697}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Baseline (Pretrained model tested on local dataset)\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "baseline_rouge = get_rouge_scores(baseline_model, local_data[\"test\"] if isinstance(local_data, dict) else local_data, tokenizer, device)\n",
    "print(\"\\n=== Baseline (Pretrained Model) Results ===\")\n",
    "print(baseline_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Skipping Training?] benitoals/my-lora found on HF. Checking for adapter config...\n",
      "\n",
      "=== LoRA Model Successfully Loaded ===\n",
      "Found LoRA adapter in benitoals/my-lora, skipping training.\n",
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: systems.\n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: of SCADAs and PLCs. However, there is a few attacks that are expected to be attacked by SCADA systems. The following attacks are known as SCADA attacks. In this scenario, we need to avoid attacks on PLC systems. These attacks can be controlled by PLC attacks, which could be used by remote control systems. It is very difficult to detect SCADA, however, it is not easy to detect attacks and attacks against SCADA. This is known for the\n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: .)\n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== After LoRA on Local Dataset ===\n",
      "{'rouge1': 15.749004491259397, 'rouge2': 2.7841409746044286, 'rougeL': 9.3724636182932, 'rougeLsum': 9.249397871341197}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train LoRA on local dataset\n",
    "local_trained_model = train_lora(baseline_model, local_data, tokenizer, local_model_repo_id)\n",
    "local_after_rouge = get_rouge_scores(local_trained_model, local_data[\"test\"] if isinstance(local_data, dict) else local_data, tokenizer, device)\n",
    "print(\"\\n=== After LoRA on Local Dataset ===\")\n",
    "print(local_after_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Skipping Training?] benitoals/my-lora-hf found on HF. Checking for adapter config...\n",
      "\n",
      "=== LoRA Model Successfully Loaded ===\n",
      "Found LoRA adapter in benitoals/my-lora-hf, skipping training.\n",
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: - - based authentication using a a- s a. s of a system. . which is based on the algorithm of authentication and authentication. using the authentication method of the remote s-s . and a method of s. - and using it using an authentication protocol. , which uses a software based system of e- . the .. based systems. e. us. a, based in which the a and\n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: of the a- - which is the e- s of the system. which a s a. of the software. . and the algorithm of which it is developed by a system. the implementation of a method of using a software based algorithm . s. - the s and a of the systems which are known as the remote control . the system is based on the ability of the control system. and it is in the processes of programming and programming algorithm.\n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: , , and the physical infrastructure which is connected to physical systems using physical and physical system based on the computing and infrastructure of the s - which are connected and connected with the e- s and a - and which the infrastructure and their infrastructure. , the implementation of a technology based infrastructure, which provide a system of computing based systems and systems. . which, including the algorithm of which which and where the a-\n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== After Training on HF Science Dataset ===\n",
      "{'rouge1': 20.656515320139402, 'rouge2': 2.5284536624071, 'rougeL': 13.610251999059756, 'rougeLsum': 13.681192959021521}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train on Hugging Face Science dataset\n",
    "huggingface_science_repo = \"CShorten/ML-ArXiv-Papers\"\n",
    "hf_data = load_dataset(huggingface_science_repo, split=\"train\").shuffle(seed=42).select(range(1000))\n",
    "hf_trained_model = train_lora(\n",
    "    baseline_model, hf_data, tokenizer, hf_model_repo_id,\n",
    "    body_key=\"title\", summary_key=\"abstract\"\n",
    ")\n",
    "hf_on_local_rouge = get_rouge_scores(hf_trained_model, local_data[\"test\"] if isinstance(local_data, dict) else local_data, tokenizer, device)\n",
    "print(\"\\n=== After Training on HF Science Dataset ===\")\n",
    "print(hf_on_local_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Skipping Training?] benitoals/my-lora-local-combined found on HF. Checking for adapter config...\n",
      "\n",
      "=== LoRA Model Successfully Loaded ===\n",
      "Found LoRA adapter in benitoals/my-lora-local-combined, skipping training.\n",
      "\n",
      "--- Debug Example 0 ---\n",
      "Input (first 300 chars): Empirical Study of PLC Authentication Protocols in Industrial Control Systems Adeen Ayub Department of Computer Science Virginia Commonwealth University Richmond, United States of America ayuba2@vcu.eduHyunguk Yoo Department of Computer Science The University of New Orleans New Orleans, United State\n",
      "Predicted Summary: logic (PLC) attacks a PLC. The PLC uses a control-logic program (PLC). In ICS, the PLCs use a software-based software that allows PLC to control physical process. The software uses the control logic of PLC (PLCs) to control the physical processes. In industrial control systems (ICS), PLC systems are controlled by PLC-s. These systems rely on a system that controls the control- logic. They use PLC and PLC in PLC, PLC software. The\n",
      "Reference Summary: Programmable logic controllers (PLCs) run a  con- trol logic  program that de nes how to control a physicalprocess such as a nuclear plant, power grid stations, and gas pipelines. Attackers target the control logic of a PLC to sabotage a physical process. Most PLCs employ password- based authentication mechanisms to prevent unauthorized remoteaccess to control logic. This paper presents an empirical study on proprietary authentication mechanisms in  ve industry-scale PLCs to understand the security-design practices of four popular ICS vendors, i.e., Allen-Bradley, Schneider Electric, Automa-tionDirect, and Siemens. The empirical study determines whether the mechanisms are vulnerable by design and can be exploited. It reveals serious design issues and vulnerabilities in authentication mechanisms, including lack of nonce, small-sized encryptionkey, weak encryption scheme, and client-side authentication. The study further con rms the  ndings empirically by creatingand testing their proof-of-concept exploits derived from MITRE ATT&CK knowledge base of adversary tactics and techniques. Unlike existing work, our study relies solely on network traf c examination and does not employ typical reverse-engineeringof binary  les (e.g., PLC  rmware) to reveal the seriousness of design problems. Moreover, the study covers PLCs from different vendors to highlight an industry-wide issue of secure PLC authentication that needs to be addressed.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 1 ---\n",
      "Input (first 300 chars): On Experimental validation of Whitelist Auto- Generation Method for Secured Programmable Logic Controllers Shintaro Fujita Dept. of Mechanical Engineering and Intelligent Systems University of Electro-Communications Tokyo, Japan shntr.fujita@uec.ac.jp Kenji Sawada Info-Powerd Energy System Research \n",
      "Predicted Summary: and Intelligent Systems. The PLC is a final fortress of control systems. In this case, we need a few attacks. However, there are a lot of attacks which could not be used by SCADAs. These attacks are very important. In these cases, the PLCs are critically sensitive to control systems, such as SCADA systems. The control systems are controlled by PLC systems. It is very difficult to detect SCADA attacks, however, it is critical to attack the controllers. The\n",
      "Reference Summary: This paper considers a whitelisting system for programmable logic controllers (PLCs). In control systems, controllers are final fortresses to continues the operation of field devices (actuators/sensors), but they are fragile with respect to malware and zero-day attacks. One of the countermeasures applicable for controllers is a whitelisting system which registers normal operations of controller behavior in a  whitelist  to detect abnormal operations via a whitelist. The previous research of the current author proposed a PLC whitelisting system with a control via a ladder diagram (LD). LD representations have a wide applicability because LDs can be implemented for all PLCs and security functions without hardware/firmware updates. However, the current status requires that all instances are manually entered in the whitelist. In this talk, we show how the setting up of the can be automatized whitelist from the PLC behavior. This paper introduces an auto-generation approach for the whitelist using sequential function chart (SFC) instead of the LD. SFC and LD are compatible representations for the PLC. Using Petri Net modeling, this paper proposes how to generate the whitelist from the SFC and how to detect abnormal operations via the whitelist. We call the SFC-based approach the model-based whitelist, the Petri Net based approach the model-based detection. Further, this paper carries out an experimental validation of the algorithms using an OpenPLC based testbed system.\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Debug Example 2 ---\n",
      "Input (first 300 chars): IEEE INTERNET OF THINGS JOURNAL, VOL. 9, NO. 15, 1 AUGUST 2022 13223 Moving Target Defense for Cyber Physical Systems Using IoT-Enabled Data Replication Jairo A. Giraldo ,Member, IEEE , Mohamad El Hariri ,Member, IEEE , and Masood Parvania ,Senior Member, IEEE Index Terms  Cyber physical systems, cy\n",
      "Predicted Summary: physical systems (CPSs) is a critical infrastructure vulnerability, which is critical to the physical infrastructure. The physical system is being controlled by software. This work is supported by the critical systems, such as physical devices, including physical networks, infrastructure and infrastructure, and the remote infrastructures, and are connected to critical devices, and is known as the CPSs. This project is based on the IoT-enabled data attacks. This paper is focused on critical data replication,\n",
      "Reference Summary: This article proposes a novel moving target defense (MTD) strategy that leverages the versatility of the Internetof Things (IoT) networks to enhance the security of cyber physical systems (CPSs) by replicating relevant sensory andcontrol signals. The replicated data are randomly selected andtransmitted to create two layers of uncertainties that reduce theability of adversaries to launch successful cyberattacks, withoutaffecting the performance of the system in a normal operation.The theoretical foundations of designing the IoT network andoptimal allocation of replicas per signal are developed for linear-time-invariant systems, and fundamental limits of uncertaintiesintroduced by the framework are calculated. The orchestration of the layers and applications integrated in the proposed framework is demonstrated in experimental implementation on a real-timewater system over a WiFi network, adopting a data-centricarchitecture. The implementation results demonstrate that theproposed framework considerably limits the impact of false-data-injection attacks, while decreasing the ability of adversaries tolearn details about the physical system operation.\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Final Model (HF + Local) ===\n",
      "{'rouge1': 23.531807800019266, 'rouge2': 4.138358449702368, 'rougeL': 13.852303413162288, 'rougeLsum': 13.801277697192694}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Fine-tune HF model on local dataset\n",
    "# Here we freeze the base (from previous training) and train only the new LoRA adapter\n",
    "final_model = train_lora(hf_trained_model, local_data, tokenizer, combined_repo_id, freeze_base=True)\n",
    "final_rouge = get_rouge_scores(final_model, local_data[\"test\"] if isinstance(local_data, dict) else local_data, tokenizer, device)\n",
    "print(\"\\n=== Final Model (HF + Local) ===\")\n",
    "print(final_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== All Four Evaluation Results =====\n",
      "1) Baseline (Pretrained Model)      => {'rouge1': 0.2525137432388697, 'rouge2': 0.12794102714632688, 'rougeL': 0.2525137432388697, 'rougeLsum': 0.2525137432388697}\n",
      "2) LoRA on Local Dataset            => {'rouge1': 15.749004491259397, 'rouge2': 2.7841409746044286, 'rougeL': 9.3724636182932, 'rougeLsum': 9.249397871341197}\n",
      "3) LoRA on HF Dataset               => {'rouge1': 20.656515320139402, 'rouge2': 2.5284536624071, 'rougeL': 13.610251999059756, 'rougeLsum': 13.681192959021521}\n",
      "4) Fine-tuned HF + Local Model      => {'rouge1': 23.531807800019266, 'rouge2': 4.138358449702368, 'rougeL': 13.852303413162288, 'rougeLsum': 13.801277697192694}\n"
     ]
    }
   ],
   "source": [
    "# Print all four results\n",
    "print(\"\\n===== All Four Evaluation Results =====\")\n",
    "print(\"1) Baseline (Pretrained Model)      =>\", baseline_rouge)\n",
    "print(\"2) LoRA on Local Dataset            =>\", local_after_rouge)\n",
    "print(\"3) LoRA on HF Dataset               =>\", hf_on_local_rouge)\n",
    "print(\"4) Fine-tuned HF + Local Model      =>\", final_rouge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
